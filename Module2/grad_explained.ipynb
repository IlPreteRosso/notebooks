{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd** \n",
    "\n",
    "It is a tool that does the calculation of derivatives via a technique called **automatic differentiation**. As quoted from the official documentation: `torch.autograd` *provides classes and functions implementing automatic differentiation of arbitrary scalar-valued functions.* \n",
    "\n",
    "Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function. As it is required during the backpropagation pass(to compute the gradient of weights w.r.t loss function) while training a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation Graph**\n",
    "\n",
    "So how does during backpropagation, PyTorch(or any other DL library for that matter) calculates gradients, it does by generating a data structure called **Computation graph**. In a complex setup where there are thousands of variables to calculate the gradient, a computation graph comes into the picture.\n",
    "\n",
    "**Computation graph** is nothing but a simple map of references of variables(or tensors) and operators(or functions) generated for a set of algebraic equations, through which autograd can traverse and trace back (to leaves) to calculate gradients.\n",
    "\n",
    "Now, as PyTorch generate these graphs during runtime in a forward pass(simple calculation of outputs from inputs), graphs are called Dynamic Computation Graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Important properties : requires_grad , grad_fn , is_leaf**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. requires_grad**\n",
    "\n",
    "The `requires_grad` attribute tells autograd to track your operations. So if you want PyTorch to create a graph corresponding to these operations, you will have to set the `requires_grad` attribute of the Tensor to True.\n",
    "\n",
    "There are 2 ways in which it can be done, either by passing it as an argument in `torch.tensor` (`requires_grad=True`)) or explicitly setting up the `requires_grad` property to True.\n",
    "\n",
    "*It is to remember that tensors with only ***float*** data types can require gradient (or ask autograd to record its operations).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 : tensor([1., 2.], dtype=torch.float64, requires_grad=True)\n",
      "t3 : tensor([1., 2.], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.DoubleTensor([1., 2.])  # dtype = torch.float16\n",
    "t2.requires_grad=True\n",
    "print(f't2 : {t2}')\n",
    "\n",
    "t3 = torch.HalfTensor([1., 2.])  # dtype = torch.float64\n",
    "t3.requires_grad=True\n",
    "print(f't3 : {t3}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points to ponder:**\n",
    "\n",
    "- The Tensors generated by applying any operations on other tensors, given that the for at least one input tensor `requires_grad = True`, then the resultant tensor will also have `requires_grad = True`.\n",
    "\n",
    "- It is also helpful when in a network we don’t want to change the gradients and hence don’t want to update the weights associated with some tensors. Just setting `require_grad = False` the tensors won’t participate in the computation graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. grad_fn**\n",
    "\n",
    "The `grad_fn` property holds the reference to the function (mathematical operator) that creates it. It is very important during a backward pass as the function here is responsible to calculate the gradient and send it to the appropriate next function in the next pass.\n",
    "\n",
    "- If `requires_grad` is set to False, `grad_fn` would be None.\n",
    "\n",
    "**1.3. is_leaf**\n",
    "\n",
    "The `is_leaf` property tells whether a tensor is a leaf node or not. Essentially leaf tensors are the tensors whom we want to accumulate the gradient and are present at the edge of the computation graph. **Only leaf Tensors will have their grad populated during a call to** `backward()`. Technically, the leaf tensors are any tensors that created by the following approaches:\n",
    "\n",
    "- Tensors resulting in operations from tensors that have `requires_grad = False` will be leaf Tensors.\n",
    "\n",
    "- Any tensor that is explicitly created by the user will be leaf Tensors. This means as they are not the result of an operation and so `grad_fn = None`.\n",
    "\n",
    "In the following example, the tensor `x` is only the leaf node. And as x is a leaf node, the `grad_fn = None` (as it is not obtained from any operations).\n",
    "\n",
    "The tensor `y` has `grad_fn` a multiplication operator since `y` is obtained from the multiplication of `a` and `x`. Similarly the case for `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x\n",
      "grad funtion = None\n",
      "is leaf = True\n",
      "True\n",
      "\n",
      "Tensor y\n",
      "grad funtion = <MulBackward0 object at 0x13a552c80>\n",
      "is leaf = False\n",
      "True\n",
      "\n",
      "Tensor z\n",
      "grad funtion = <AddBackward0 object at 0x13a552c80>\n",
      "is leaf = False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# leaf nodes\n",
    "x = torch.tensor(3., requires_grad=True)\n",
    "a = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "y = a * x\n",
    "# y is non-leaf\n",
    "y.retain_grad()\n",
    "\n",
    "z = y + b\n",
    "\n",
    "print(\"Tensor x\")\n",
    "print(f'grad funtion = {x.grad_fn}')\n",
    "print(f'is leaf = {x.is_leaf}')\n",
    "print(x.requires_grad)\n",
    "\n",
    "print(\"\\nTensor y\")\n",
    "print(f'grad funtion = {y.grad_fn}')\n",
    "print(f'is leaf = {y.is_leaf}')\n",
    "print(y.requires_grad)\n",
    "\n",
    "print(\"\\nTensor z\")\n",
    "print(f'grad funtion = {z.grad_fn}')\n",
    "print(f'is leaf = {z.is_leaf}')\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. backward()**\n",
    "\n",
    "The signature for `backward` is `backward(gradient=None, retain_graph=None, create_graph=False)`.\n",
    "\n",
    "This the most important of the tensor methods present here. It computes the gradient of current tensor w.r.t. graph leaves. It is responsible to calculate the gradient during a backward pass.\n",
    "\n",
    "These are the typical steps involved in gradient calculation during a backward pass:\n",
    "\n",
    "1. The backward function takes an **incoming gradient** from the part of the network in front of it.\n",
    "\n",
    "2. Then it calculates the local gradient at a particular tensor.\n",
    "\n",
    "3. Then it multiplies the local gradient to with incoming gradient\n",
    "\n",
    "4. Finally, forwards the computed gradient to the tensor’s inputs by invoking the backward method of the `grad_fn` of their inputs or simply save the gradient in `grad` property for leaf nodes.\n",
    "\n",
    "\\\n",
    "Recall that z = y + b = (a*x) + b, we easily compute that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = a = 4 \n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial a} = x = 3 \n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x\n",
      "grad funtion = None\n",
      "\n",
      "Tensor a\n",
      "grad funtion = None\n",
      "\n",
      "Tensor b\n",
      "grad funtion = None\n",
      "\n",
      "Tensor y\n",
      "grad funtion = <MulBackward0 object at 0x13aa16560>\n",
      "\n",
      "Tensor z\n",
      "grad funtion = <AddBackward0 object at 0x13aa16560>\n",
      "\n",
      "\n",
      "dz/dx: tensor(4.)\n",
      "dz/da: tensor(3.)\n",
      "dz/db: tensor(1.)\n",
      "(normally we don't commpute grad of non-leaf nodes) dz/dy: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "z.backward()    # imagine pushing back to the leaf nodes\n",
    "\n",
    "print(\"Tensor x\")\n",
    "print(f'grad funtion = {x.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor a\")\n",
    "print(f'grad funtion = {a.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor b\")\n",
    "print(f'grad funtion = {b.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor y\")\n",
    "print(f'grad funtion = {y.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor z\")\n",
    "print(f'grad funtion = {z.grad_fn}')\n",
    "\n",
    "print('\\n')\n",
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) \n",
    "print('dz/db:', b.grad) \n",
    "print('(normally we don\\'t compute grad of non-leaf nodes) dz/dy:', y.grad) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose in the above example, when calling `z.backward`. The `grad_fn` of `z` is `<AddBackward>`.\n",
    "\n",
    "1. The backward function of `<AddBackward>` takes a default input tensor as `torch.tensor([1.])`.\n",
    "\n",
    "2. Then it calculates gradient for `y` and `b`. For both, the gradients will be [1.] as the operator is an addition function.\n",
    "\n",
    "3. The gradient is multiplied with the incoming tensor i.e. [1.] * [1.].\n",
    "\n",
    "4. Now for `b`, the `grad_fn = None` so the gradient computed directly will get stored in `grad` property of tensor `b`. And for tensor `y` the backward function passes the gradient to its input tensor’s `grad_fn` (i.e. `<MulBackward>` of `y` since it is formed after the multiplication of `x` and `a`)\n",
    "\n",
    "5. Similarly, the backward function will be called for y’s `<MulBackward>` with an input gradient from `<AddBackward>` i.e. [1.] in this case.\n",
    "\n",
    "As noticed, the backward function is recursively called throughout the graph as we backtrack. You can access the gradients by calling the `grad` attribute of Tensor.\n",
    "\n",
    ">Note: backward function only calculates gradients by going over an already made backward graph. The backward graph is as discussed generated during a forward pass only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Calling backward() on non-scaler tensor**\n",
    "\n",
    "For a vector-valued tensor, the backward function gives a `Runtime error: grad can be implicitly created only for scalar outputs`.\n",
    "\n",
    "This is because for a non-scalar tensor a jacobian-vector is to be computed and then the `backward` expects incoming gradients as it’s input (usually the gradient of the differentiated function w.r.t. corresponding tensors). Hence the `backward` *expects incoming gradient a Tensor of the same size as the current tensor*, then it’ll able to backpropagate.\n",
    "\n",
    "**So either you can pass the tensor of the same shape,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 5.], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "a = torch.tensor([4.,2.], requires_grad=True)\n",
    "z = x + a\n",
    "\n",
    "display(z)  # Broadcasting, see Ch. 02a\n",
    "z.backward(torch.tensor([1., 1.])) # passing a gradient to backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(2.)\n",
      "dz/da: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note : If you’ll pass non-ones tensor in backward, the gradients will get scaled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 5.], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "a = torch.tensor([4.,2.], requires_grad=True)\n",
    "z = x + a\n",
    "\n",
    "display(z)  # Broadcasting, see Ch. 02a\n",
    "z.backward(torch.tensor([1000., 1000.])) # passing a gradient to backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(2000.)\n",
      "dz/da: tensor([1000., 1000.])\n"
     ]
    }
   ],
   "source": [
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or simply change the size of the current tensor to torch.Size([]) as expected by backward.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "a = torch.tensor([4.,2.], requires_grad=True)\n",
    "z = x + a\n",
    "z = z.mean()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(1.)\n",
      "dz/da: tensor([0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "a = torch.tensor([4.,2.], requires_grad=True)\n",
    "z = x + a\n",
    "z = z.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(2.)\n",
      "dz/da: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow, now we have understood the basic functioning of Autograd in PyTorch along with functions to implement that. But we’ll wait now and get back to our Computation Graph diagram for the same equation to concretize the concept.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the DCG of the same Example (Example 1) when we have `require_grad = True`\n",
    "\n",
    "- The tensors in green are leaf nodes.\n",
    "\n",
    "- Tensors in yellow are intermediate nodes.\n",
    "\n",
    "- `MulBackward` and `AddBackward` are two `grad_fn` for `y` and `z` respectively.\n",
    "\n",
    "- `grad` attribute stores the value of calculated gradients.\n",
    "\n",
    "![alt text](/Users/ilpreterosso/GitHub/VSCode/NN101/notebooks/Module2/img/Screenshot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. register_hook()**\n",
    "\n",
    "The hook will be called every time a gradient (w.r.t a Tensor) is computed. The hook should have the following signature:\n",
    "\n",
    "`hook(grad) -> Tensor or None`\n",
    "\n",
    "This function returns a *handle* with a method `handle.remove()` that removes the hook from the module.\n",
    "\n",
    "**So, the hook can take the value of grad and can return a new value or perform operations with the value.**\n",
    "\n",
    "This is the best part, this can help to\n",
    "\n",
    "- Modify the `grad` on the fly during a backward pass without waiting for the pass to be completed. This can influence our ways to calculate the gradient in a graph.\n",
    "\n",
    "- Debug the code for the flow of gradients in your graph. Identifying gradients at each step even for non-leaf nodes.\n",
    "\n",
    "Looking at an example from the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "display(v.grad)\n",
    "h.remove() # removes the hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
