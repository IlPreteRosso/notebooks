{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd** \n",
    "\n",
    "It is a tool that does the calculation of derivatives via a technique called **automatic differentiation**. As quoted from the official documentation: `torch.autograd` *provides classes and functions implementing automatic differentiation of arbitrary scalar-valued functions.* \n",
    "\n",
    "Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function. As it is required during the backpropagation pass(to compute the gradient of weights w.r.t loss function) while training a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation Graph**\n",
    "\n",
    "So how does during backpropagation, PyTorch(or any other DL library for that matter) calculates gradients, it does by generating a data structure called **Computation graph**. In a complex setup where there are thousands of variables to calculate the gradient, a computation graph comes into the picture.\n",
    "\n",
    "**Computation graph** is nothing but a simple map of references of variables(or tensors) and operators(or functions) generated for a set of algebraic equations, through which autograd can traverse and trace back (to leaves) to calculate gradients.\n",
    "\n",
    "Now, as PyTorch generate these graphs during runtime in a forward pass(simple calculation of outputs from inputs), graphs are called Dynamic Computation Graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Important properties : requires_grad , grad_fn , is_leaf**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. requires_grad**\n",
    "\n",
    "The `requires_grad` attribute tells autograd to track your operations. So if you want PyTorch to create a graph corresponding to these operations, you will have to set the `requires_grad` attribute of the Tensor to True.\n",
    "\n",
    "There are 2 ways in which it can be done, either by passing it as an argument in `torch.tensor` (Works only for singleton `pytorch.tensor`, not `python.Tensor`) or explicitly setting up the `requires_grad` property to True.\n",
    "\n",
    "*It is to remember that tensors with only ***float*** data types can require gradient (or ask autograd to record its operations).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 : tensor([1., 2.], dtype=torch.float64, requires_grad=True)\n",
      "t3 : tensor([1., 2.], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.DoubleTensor([1., 2.])  # dtype = torch.float16\n",
    "t2.requires_grad=True\n",
    "print(f't2 : {t2}')\n",
    "\n",
    "t3 = torch.HalfTensor([1., 2.])  # dtype = torch.float64\n",
    "t3.requires_grad=True\n",
    "print(f't3 : {t3}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points to ponder:**\n",
    "\n",
    "- The Tensors generated by applying any operations on other tensors, given that the for at least one input tensor `requires_grad = True`, then the resultant tensor will also have `requires_grad = True`.\n",
    "\n",
    "- It is also helpful when in a network we don’t want to change the gradients and hence don’t want to update the weights associated with some tensors. Just setting `require_grad = False` the tensors won’t participate in the computation graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. grad_fn**\n",
    "\n",
    "The `grad_fn` property holds the reference to the function (mathematical operator) that creates it. It is very important during a backward pass as the function here is responsible to calculate the gradient and send it to the appropriate next function in the next pass.\n",
    "\n",
    "- If `requires_grad` is set to False, `grad_fn` would be None.\n",
    "\n",
    "**1.3. is_leaf**\n",
    "\n",
    "The `is_leaf` property tells whether a tensor is a leaf node or not. Essentially leaf tensors are the tensors whom we want to accumulate the gradient and are present at the edge of the computation graph. **Only leaf Tensors will have their grad populated during a call to** `backward()`. Technically, the leaf tensors are any tensors that created by the following approaches:\n",
    "\n",
    "- Tensors resulting in operations from tensors that have `requires_grad = False` will be leaf Tensors.\n",
    "\n",
    "- Any tensor that is explicitly created by the user will be leaf Tensors. This means as they are not the result of an operation and so `grad_fn = None`.\n",
    "\n",
    "In the following example, the tensor `x` is only the leaf node. And as x is a leaf node, the `grad_fn = None` (as it is not obtained from any operations).\n",
    "\n",
    "The tensor `y` has `grad_fn` a multiplication operator since `y` is obtained from the multiplication of `a` and `x`. Similarly the case for `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x\n",
      "grad funtion = None\n",
      "is leaf = True\n",
      "True\n",
      "\n",
      "Tensor y\n",
      "grad funtion = <MulBackward0 object at 0x13a552c80>\n",
      "is leaf = False\n",
      "True\n",
      "\n",
      "Tensor z\n",
      "grad funtion = <AddBackward0 object at 0x13a552c80>\n",
      "is leaf = False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "a = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "# y is non-leaf\n",
    "y = a * x\n",
    "y.retain_grad()\n",
    "\n",
    "z = y + b\n",
    "\n",
    "print(\"Tensor x\")\n",
    "print(f'grad funtion = {x.grad_fn}')\n",
    "print(f'is leaf = {x.is_leaf}')\n",
    "print(x.requires_grad)\n",
    "\n",
    "print(\"\\nTensor y\")\n",
    "print(f'grad funtion = {y.grad_fn}')\n",
    "print(f'is leaf = {y.is_leaf}')\n",
    "print(y.requires_grad)\n",
    "\n",
    "print(\"\\nTensor z\")\n",
    "print(f'grad funtion = {z.grad_fn}')\n",
    "print(f'is leaf = {z.is_leaf}')\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. backward()**\n",
    "\n",
    "The signature for `backward` is `backward(gradient=None, retain_graph=None, create_graph=False)`.\n",
    "\n",
    "This the most important of the tensor methods present here. It computes the gradient of current tensor w.r.t. graph leaves. It is responsible to calculate the gradient during a backward pass.\n",
    "\n",
    "These are the typical steps involved in gradient calculation during a backward pass:\n",
    "\n",
    "1. The backward function takes an **incoming gradient** from the part of the network in front of it.\n",
    "\n",
    "2. Then it calculates the local gradient at a particular tensor.\n",
    "\n",
    "3. Then it multiplies the local gradient to with incoming gradient\n",
    "\n",
    "4. Finally, forwards the computed gradient to the tensor’s inputs by invoking the backward method of the `grad_fn` of their inputs or simply save the gradient in `grad` property for leaf nodes.\n",
    "\n",
    "\\\n",
    "Recall that z = y + b = (a*x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x\n",
      "grad funtion = None\n",
      "\n",
      "Tensor a\n",
      "grad funtion = None\n",
      "\n",
      "Tensor b\n",
      "grad funtion = None\n",
      "\n",
      "Tensor y\n",
      "grad funtion = <MulBackward0 object at 0x13aa16560>\n",
      "\n",
      "Tensor z\n",
      "grad funtion = <AddBackward0 object at 0x13aa16560>\n",
      "\n",
      "\n",
      "dz/dx: tensor(4.)\n",
      "dz/da: tensor(3.)\n",
      "dz/db: tensor(1.)\n",
      "(normally we don't commpute grad of non-leaf nodes) dz/dy: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "\n",
    "print(\"Tensor x\")\n",
    "print(f'grad funtion = {x.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor a\")\n",
    "print(f'grad funtion = {a.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor b\")\n",
    "print(f'grad funtion = {b.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor y\")\n",
    "print(f'grad funtion = {y.grad_fn}')\n",
    "\n",
    "print(\"\\nTensor z\")\n",
    "print(f'grad funtion = {z.grad_fn}')\n",
    "\n",
    "print('\\n')\n",
    "print('dz/dx:', x.grad) \n",
    "print('dz/da:', a.grad) \n",
    "print('dz/db:', b.grad) \n",
    "print('(normally we don\\'t commpute grad of non-leaf nodes) dz/dy:', y.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
